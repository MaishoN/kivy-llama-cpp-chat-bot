[model]
llama_cli_path = llama.cpp/build/bin/llama-cli
model_name = gemma-2-9b-it-Q6_K.gguf
models_dir = models
states_dir = states
state_file = state_8.txt
use_system_prompt = 0
load_chat_history = 1
save_chat_history = 1
system_prompt_mark = system
system_prompt = You are a helpful assistant
user_prompt_mark = user
model_prompt_mark = model
start_token = <start_of_turn>
end_token = <end_of_turn>
user_default_message = {0}
state = states/state_1.txt

